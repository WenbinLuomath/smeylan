@article{kurumadaMeylanFrank2013,
 abstract = {Word frequencies in natural language follow a highly skewed Zipfian distribution, but the consequences of this distribution for language acquisition are only beginning to be understood. Typically, learning experiments that are meant to simulate language acquisition use uniform word frequency distributions. We examine the effects of Zipfian distributions using two artificial language paradigms-a standard forced-choice task and a new orthographic segmentation task in which participants click on the boundaries between words in contexts. Our data show that learners can identify word forms robustly across widely varying frequency distributions. In addition, although performance in recognizing individual words is predicted best by their frequency, a Zipfian distribution facilitates word segmentation in context: The presence of high-frequency words creates more chances for learners to apply their knowledge in processing new sentences. We find that computational models that implement " chunking" are more effective than " transition finding" models at reproducing this pattern of performance. ?? 2013 Elsevier B.V.},
 author = {Kurumada, Chigusa and Meylan, Stephan C. and Frank, Michael C.},
 authors_url = {http://langcog.stanford.edu/papers/KMF-cognition2013.pdf},
 code_url = {https://github.com/smeylan/PARSER_zipfseg_analysis},
 doi = {10.1016/j.cognition.2013.02.002},
 issn = {00100277},
 journal = {Cognition},
 journal_url = {https://www.sciencedirect.com/science/article/pii/S0010027713000309},
 number = {3},
 pages = {439--453},
 pmid = {23558340},
 publisher = {Elsevier B.V.},
 title = {{Zipfian frequency distributions facilitate word segmentation in context}},
 url = {http://langcog.stanford.edu/papers/KMF-cognition2013.pdf},
 volume = {127},
 year = {2013}
}

@article{meylanBergelson2021,
 author = {Meylan, Stephan C and Bergelson, Elika},
 journal = {Annual Review of Linguistics},
 journal_url = {https://www.annualreviews.org/doi/abs/10.1146/annurev-linguistics-031220-011146},
 preprint_url = {https://psyarxiv.com/4zxfp/},
 title = {{Learning through processing: Towards an integrated approach to early word learning}},
 volume = {8},
 year = {2022}
}

@article{MeylanEtAl2021telephone,
 abstract = {{\textcopyright} 2021 Elsevier B.V. Spoken communication occurs in a “noisy channel” characterized by high levels of environmental noise, variability within and between speakers, and lexical and syntactic ambiguity. Given these properties of the received linguistic input, robust spoken word recognition—and language processing more generally—relies heavily on listeners' prior knowledge to evaluate whether candidate interpretations of that input are more or less likely. Here we compare several broad-coverage probabilistic generative language models in their ability to capture human linguistic expectations. Serial reproduction, an experimental paradigm where spoken utterances are reproduced by successive participants similar to the children's game of “Telephone,” is used to elicit a sample that reflects the linguistic expectations of English-speaking adults. When we evaluate a suite of probabilistic generative language models against the yielded chains of utterances, we find that those models that make use of abstract representations of preceding linguistic context (i.e., phrase structure) best predict the changes made by people in the course of serial reproduction. A logistic regression model predicting which words in an utterance are most likely to be lost or changed in the course of spoken transmission corroborates this result. We interpret these findings in light of research highlighting the interaction of memory-based constraints and representations in language processing.},
 author = {Meylan, S.C. and Nair, S. and Griffiths, T.L.},
 code_url = {https://github.com/smeylan/telephone-analysis-public/},
 doi = {10.1016/j.cognition.2020.104553},
 issn = {18737838},
 journal = {Cognition},
 journal_url = {https://www.sciencedirect.com/science/article/abs/pii/S0010027720303723},
 osf_url = {osf.io/3hws2},
 preprint_url = {https://arxiv.org/abs/2101.09788},
 title = {{Evaluating models of robust word recognition with serial reproduction}},
 url = {https://arxiv.org/abs/2101.09788},
 volume = {210},
 year = {2021}
}

@article{meylanFousheeWongEtAlUnderReview,
 abstract = {Children's early speech often bears little resemblance to adult speech in form or content, and yet caregivers often find meaning in young children's utterances. Precisely how caregivers are able to do this remains poorly understood. We propose that successful early communication (an essential building block of language development) relies not just on children's growing linguistic knowledge, but also on adults' sophisticated inferences. These inferences, we further propose, are optimized for fine-grained details of how children speak. We evaluate these ideas using a set of candidate computational models of spoken word recognition based on deep learning and Bayesian inference, which instantiate competing hypotheses regarding the information sources used by adults to understand children. We find that the best-performing models (evaluated on datasets of adult interpretations of child speech) are those that have strong prior expectations about what children are likely to want to communicate, rather than the actual phonetic contents of what children say. We further find that adults' behavior is best characterized as well-tuned to specific children: the more closely a word recognition model is tuned to the particulars of an individual child's actual linguistic behavior, the better it predicts adults' inferences about what the child has said. These results offer a comprehensive investigation into the role of caregivers as child-directed listeners, with broader consequences for theories of language acquisition.},
 archiveprefix = {arXiv},
 arxivid = {2206.07807},
 author = {Meylan, Stephan C. and Foushee, Ruthe and Wong, Nicole H. and Bergelson, Elika and Levy, Roger P.},
 code_url = {https://github.com/smeylan/child-directed-listening/},
 eprint = {2206.07807},
 osf_url = {https://osf.io/v7c3e/},
 preprint_url = {https://arxiv.org/abs/2206.07807},
 title = {{How Adults Understand What Young Children Say}},
 url = {http://arxiv.org/abs/2206.07807}
}

@article{meylanFrankEtAl2016,
 author = {Meylan, Stephan C and Frank, Michael C and Roy, Brandon C and Levy, Roger},
 code_url = {https://github.com/smeylan/determiner_learning},
 data_url = {https://childes.talkbank.org/derived/meylan.zip},
 journal = {Psychological Science},
 journal_url = {https://journals.sagepub.com/doi/10.1177/0956797616677753},
 number = {2},
 pages = {181--192},
 preprint_url = {http://langcog.stanford.edu/papers_new/meylan-inpress-psychsci.pdf},
 title = {{The emergence of an abstract grammatical category in children's early speech}},
 url = {http://langcog.stanford.edu/papers_new/meylan-inpress-psychsci.pdf},
 volume = {28},
 year = {2017}
}

@article{meylanGriffiths2021predictability,
 author = {Meylan, Stephan C. and Griffiths, Thomas L.},
 code_url = {https://github.com/smeylan/frequency-vs-info-content/},
 isbn = {0000000310},
 journal = {Cognitive Science},
 journal_url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.12983},
 osf_url = {https://osf.io/np2bs},
 preprint_url = {https://psyarxiv.com/6832r/},
 title = {{The Challenges of Large-Scale, Web-based Language Datasets: Word Length and Predictability Revisited}},
 url = {https://psyarxiv.com/6832r/},
 year = {2021}
}

@article{peekbank2022,
 author = {Zettersten, M. and Yurovsky, D. and Xu, T.L. and Uner, S. and Tsui, A. and Schneider, R.M. and Saleh, A. and Meylan, S.C. and Marchman, V.A. and Mankewitz, J. and MacDonald, K. and Long, B. and Lewis, M. and Kachergis, G.\ and Handa, K. and DeMayo, B. and Carstensen, A. and Braginsky, M. and Boyce, V. and Bhatt, N. and Bergey, C. and Frank, M.C.},
 code_url = {https://github.com/orgs/langcog/teams/peekbank/repositories},
 journal = {Behavior Research Methods},
 osf_url = {https://osf.io/pr6wu/},
 preprint_url = {https://psyarxiv.com/tgnzv/},
 project_website_url = {https://peekbank.stanford.edu},
 title = {{Peekbank: An open, large-scale repository for developmental eye-tracking data of children's word recognition}},
 year = {2022}
}

@article{sanchezEtAl2019,
 abstract = {The Child Language Data Exchange System (CHILDES) has played a critical role in research on child language development, particularly in characterizing the early language learning environment. Access to these data can be both complex for novices and difficult to automate for advanced users, however. To address these issues, we introduce childes-db, a database-formatted mirror of CHILDES that improves data accessibility and usability by offering novel interfaces, including browsable web applications and an R application programming interface (API). Along with versioned infrastructure that facilitates reproducibility of past analyses, these interfaces lower barriers to analyzing naturalistic parent–child language, allowing for a wider range of researchers in language and cognitive development to easily leverage CHILDES in their work.},
 author = {Sanchez, Alessandro and Meylan, Stephan C. and Braginsky, Mika and MacDonald, Kyle E. and Yurovsky, Daniel and Frank, Michael C.},
 code_url = {https://github.com/langcog/childesr},
 doi = {10.3758/s13428-018-1176-7},
 issn = {15543528},
 journal = {Behavior Research Methods},
 journal_url = {https://link.springer.com/article/10.3758/s13428-018-1176-7},
 number = {4},
 pages = {1928--1941},
 preprint_url = {https://psyarxiv.com/93mwx/},
 project_website_url = {http://childes-db.stanford.edu/},
 publisher = {Behavior Research Methods},
 title = {{childes-db: A flexible and reproducible interface to the child language data exchange system}},
 url = {https://stephanmeylan.com/resources/sanchezChildesDB.pdf},
 volume = {51},
 year = {2019}
}

