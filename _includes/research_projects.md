In my research, I use computational models to investigate human language processing and first language acquisition. I collect data from web experiments on [Amazon Mechanical Turk](https://www.mturk.com/mturk/welcome) and make extensive use of natural language corpora including [CHILDES](http://childes.psy.cmu.edu/), [Speechome](http://www.media.mit.edu/cogmac/projects/hsp.html), SWITCHBOARD, the [British National Corpus](http://www.natcorp.ox.ac.uk/), OPUS, and [Google N-grams](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html).

Five specific questions I'm working on right now are:

1. How rich are children's early representations of language structure? What can we infer from their early use of articles ("a", "an," and "the") about whether there's an innate "noun" category? (joint work with [Michael C. Frank](http://web.stanford.edu/~mcfrank/), [Roger Levy](http://cpl.ucsd.edu/), and [Brandon Roy](http://alumni.media.mit.edu/~bcroy/))
2. Can we track early word learning—a larger vocabulary with higher temporal resolution—using smartphones? (joint work with Michael C. Frank and Mika Bragisky) 
3. How does the phonological structure of low frequency words facilitate word recognition? (joint work with [Tom Griffiths](http://cocosci.berkeley.edu/tom/index.html))
4. How do children infer that there are multiple distinct senses for a given word (e.g., a "bat" can refer to a flying mammal or a piece of sports equipment)? How do they determine how senses relate to one another, and can new words can be used in similar ways? (joint work with [Mahesh Srinivasan](http://psychology.berkeley.edu/people/mahesh-srinivasan) and Tom Griffiths)