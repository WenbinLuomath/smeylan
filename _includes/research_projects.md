In my research, I use computational models of concept learning and language processing coupled with distributional represenations of semantics. I collect data from web experiments on [Amazon Mechanical Turk](https://www.mturk.com/mturk/welcome) and make extensive use of natural language corpora including [CHILDES](http://childes.psy.cmu.edu/), [Speechome](http://www.media.mit.edu/cogmac/projects/hsp.html), SWITCHBOARD, the [British National Corpus](http://www.natcorp.ox.ac.uk/), and [Google N-grams](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html).

Four specific questions I'm working on right now are:

1. How rich are children's early representations of language structure? What can we infer from their early use of articles ("a", "an," and "the") about whether there's an innate "noun" category? (joint work with [Michael C. Frank](http://web.stanford.edu/~mcfrank/), [Roger Levy](http://cpl.ucsd.edu/), and [Brandon Roy](http://alumni.media.mit.edu/~bcroy/))
2. What happens to a word--besides shortening--when it becomes highly predictable in a specific context?  (joint work with [Tom Griffiths](http://cocosci.berkeley.edu/tom/index.html))
3. How can children infer that there are multiple distinct senses for a given word (e.g., a "bat" can refer to a flying mammal or a piece of sports equipment)? How do they determine how senses relate to one another, and can new words can be used in similar ways? (joint work with [Mahesh Srinivasan](http://psychology.berkeley.edu/people/mahesh-srinivasan) and Tom Griffiths)
4. How might children learn word meanings using co-occurrence statistics for 1) words with other words and 2) words with things in their environment? (joint work with Tom Griffiths)